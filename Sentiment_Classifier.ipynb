{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Sentiment_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ0Nte8dpyJp"
      },
      "source": [
        "The first 3 code cells of this notebook are related to using Google Colab. This first cell gives Colab access to my Google Drive contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3iGDffpplXO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZAc-gyIp7Ol"
      },
      "source": [
        "I'm changing the current directory to the directory of this notebook in my Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL69KA3RpzT2"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5e62pHzqFjR"
      },
      "source": [
        "Colab doesn't have the transformers package installed so this will install it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1qwdufxFoxz"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2Wv8QKiFaQm"
      },
      "source": [
        "from nltk.tree import Tree\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWpxFCSEoYle"
      },
      "source": [
        "Here is a link to the data. Unzip it into the same directory as this notebook.\n",
        "\n",
        "https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
        "\n",
        "The data is in a tree format with 5 categories of sentiment. The following function reads in the data and flattens each tree into a string. It also reduces the 5 categories to just 2 (postive and negative)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LvRjaxXFaQ_"
      },
      "source": [
        "def sentiment_treebank_reader(filename):\n",
        "    with open(filename, encoding='utf8') as f:\n",
        "        X, y = [], []\n",
        "        for line in f:\n",
        "            tree = Tree.fromstring(line)\n",
        "            label = int(tree.label())\n",
        "            string = \" \".join(tree.leaves())\n",
        "\n",
        "            if label == 0 or label == 1: \n",
        "                y.append(0)\n",
        "                X.append(string)\n",
        "                \n",
        "            elif label == 3 or label == 4:\n",
        "                y.append(1)\n",
        "                X.append(string)\n",
        "    return X, y"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loD5HyuOFaRA"
      },
      "source": [
        "X_str_dev, y_dev = sentiment_treebank_reader('trees/dev.txt')\n",
        "X_str_train, y_train = sentiment_treebank_reader('trees/train.txt')\n",
        "X_str_test, y_test = sentiment_treebank_reader('trees/test.txt')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dGWqR8atfoH"
      },
      "source": [
        "We now define a dataset class in which the data consists of Bert-tokenized versions of the string data we just read in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf2IVqAxFaRB"
      },
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, strings, labels):\n",
        "        self.strings = strings\n",
        "        self.labels = labels\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        string = self.strings[index]\n",
        "        label = self.labels[index]\n",
        "        \n",
        "        encoding = self.tokenizer.encode_plus(string, \n",
        "                    add_special_tokens=True, return_attention_mask=True, padding='max_length')\n",
        "        return (\n",
        "                torch.tensor(encoding['input_ids']).to(device), \n",
        "                torch.tensor(encoding['attention_mask']).to(device), \n",
        "                torch.tensor(label, dtype=torch.long).to(device)\n",
        "        )\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.strings)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A80WZfWhFaRB"
      },
      "source": [
        "train_set = SentimentDataset(X_str_train, y_train)\n",
        "dev_set = SentimentDataset(X_str_dev, y_dev)\n",
        "test_set = SentimentDataset(X_str_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16)\n",
        "dev_loader = DataLoader(dev_set, batch_size=16)\n",
        "test_loader = DataLoader(test_set, batch_size=16)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckg0hVVQuRKM"
      },
      "source": [
        "As in the paper, the model is Bert with a classifier on the [CLS] output. We will then fine-tune the entire model on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfqP5ePcFaRC"
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.classifier_layer = nn.Linear(768, 2)\n",
        "    \n",
        "    def forward(self, indices, mask):\n",
        "        cls_output = self.bert(indices, attention_mask=mask)['pooler_output']\n",
        "        return self.classifier_layer(cls_output)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVm2EP5sFaRC"
      },
      "source": [
        "model = SentimentClassifier()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zG-E-iSuyGZ"
      },
      "source": [
        "This helper function evaluates the loss and accuracy on a given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a3KUePGfIVo"
      },
      "source": [
        "def evaluate_model(model, data_loader, criterion, length):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        for inputs, masks, labels in data_loader:\n",
        "            outputs = model(inputs, masks)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss, correct_predictions.item() / length"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyG_ZIuYzwue"
      },
      "source": [
        "Here is the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA1R3JHRFaRC",
        "outputId": "60f56b8e-534c-4585-b938-1ed4c118f213"
      },
      "source": [
        "for epoch in range(2):\n",
        "    print('Epoch', epoch + 1)\n",
        "    model.train()\n",
        "\n",
        "    for inputs, masks, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, masks)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss, train_acc = evaluate_model(model, train_loader, criterion, len(train_set))\n",
        "    dev_loss, dev_acc = evaluate_model(model, dev_loader, criterion, len(dev_set))\n",
        "    print('Train loss', train_loss, '  accuracy', train_acc)\n",
        "    print('Dev   loss', dev_loss, '  accuracy', dev_acc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Train loss 218.84837936796248   accuracy 0.7547687861271676\n",
            "Dev   loss 26.759268052875996   accuracy 0.7568807339449541\n",
            "Epoch 2\n",
            "Train loss 87.45118309464306   accuracy 0.926878612716763\n",
            "Dev   loss 16.714550217613578   accuracy 0.8899082568807339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj9BuVtGgGOk",
        "outputId": "fe1aa038-17f6-41f1-d0df-5fccf3e624db"
      },
      "source": [
        "_, test_acc = evaluate_model(model, test_loader, criterion, len(test_set))\n",
        "print('Test accuracy', test_acc)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 0.8786381109280615\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}